
# 开头

最终的目的，绘制烟花和音频动效

以下内容属于个人理解猜测，如有错误还请各位大佬指出

# 图形学api

## html绘图和图形学api绘图的区别

绘制图像的最终目的都是计算出屏幕每个像素点的颜色然后交给显卡输出到显示器，前者在cpu完成计算，后者在gpu完成计算。chrome底层渲染引擎用的skia，像大家熟知的canvas一样也提供点线面的基本绘制命令。图形学api则由各家硬件厂商提供，目前主要有directX、vulkan、metal三大图形api，opengl因不适合现代显卡而退出群聊。

## 图形学api绘图方式

在gpu绘图的流程中，需要我们提供一堆坐标数据，通常以三角形的方式组织数据，顶点着色器输出每个顶点的坐标，然后光栅化找出想要绘制的点/线/三角面所占据的屏幕像素点，片元着色器输出这些像素点的颜色信息。

所以编写顶点着色器和片元着色器代码返回顶点坐标和颜色数据就是gpu的绘图方式，颜色数据可以直接返回顶点坐标作为rgb值，中间的像素gpu会进行线性插值，复杂的情况还需要纹理、贴图、材质、灯光等图形学知识。现代的显卡已经足够强大可以不采用光栅化策略，而是在每帧内计算每个像素点受到场景中光源发出的光线经过物体多次反射折射后的叠加影响，这就是传说中的光线追踪。

## NDC空间

gpu绘图的“画布”相比canvas画布多了第三维z轴坐标并且坐标单位不是像素，z轴方向指向屏幕里面，画布右上角为(1,1)，左下角为(-1,-1)，z坐标对于描点没有实际意义，不过z坐标在-1～1之外的顶点会被舍弃，这样形成的画布空间被称为NDC空间(normalized device coordinate)

## 坐标转换

有了NDC空间就可以直接将脑海中的场景用ndc坐标表示出来，但这样显然很麻烦，为了知道场景中以不同位置不同角度看到的景象，我们引入相机的概念。以场景的世界坐标系来构建顶点，建立物体的顶点坐标到ndc坐标的映射关系。

# 矩阵变换

> 物体的顶点坐标到ndc坐标的映射用一个矩阵表示，矩阵可以将一个点转化为另一个点  
矩阵MxN的计算：M矩阵的m行与N矩阵的n列的各项元素分别相乘的和得到新矩阵的m行n列元素  
矩阵乘法满足结合率不满足交换率

## 平移、旋转、缩放

将一个点p1(x1,y1,z1)在xyz方向上分别平移dx,dy,dz个单位，不难看出平移后的p2坐标为(x1+dx,y1+dy,z1+dz),可以用一个4x4的矩阵表示这一变化

```js
列向量写法：

1 0 0 dx       x     x+dx  
0 1 0 dy   *   y  =  y+dy  
0 0 1 dz       z     z+dz  
0 0 0 1        1     1  

行向量写法：

              1  0  0  0       
x y z 1   *   0  1  0  0  =  x+dx  y+dy  z+dz 1
              0  0  1  0       
              dx dy dz 1      
```

> 不知道为什么大家通常使用列向量写法，矩阵的数据结构是一维数组，着色器语言的矩阵`Mat4`也是按列的方式录入数据

将一个模型进行缩放，也就是将每个顶点进行缩放，缩放矩阵如下

```js
sx 0  0  0
0  sy 0  0
0  0  sz 0
0  1  1  1
```
> 一个顶点p1(x1,y1,z1)应用缩放矩阵后会得到p2(x1/sx,y1/sy,z1/sz)，这跟平移一样也能直观的看出

绕y轴旋转的矩阵如下

```js
const c = Math.cos(theta)
const s = Math.sin(theta);

c  0  s  0
0  1  0  0
-s 0  c  0
0  0  0  1
```

> 这里简单推导一下(世界坐标系通常为右手坐标系，z轴指向屏幕外侧，以右手螺旋定则大拇指指向旋转轴正方向其余四指则为旋转方向规定为旋转正方向)  
> ![旋转矩阵推导]("url")

通过对模型顶点的旋转平移缩放就能设置模型在场景中的位置和"姿态"，这个变化矩阵被称作模型

## 空间转换
空间转换讨论如何求出某坐标系下的一点在另一坐标系下的坐标，实际应用中会将世界坐标系下的顶点点转化为相机坐标系的坐标(就像画家写生是以人眼坐标系位画)

通常规定相机用-z方向去看物体，相机up方向为y方向，up方向决定观察者是"正着看还是歪头看“

嘉定

# 普通canvas绘制旋转的立方体




# webgpu基本使用
# webgpu简单封装
# 烟花绘制思路